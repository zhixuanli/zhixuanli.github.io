<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>MUVA</title>
<!--  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">-->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">MUVA: A New Large-Scale Benchmark for Multi-view Amodal Instance Segmentation in the Shopping Scenario</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://zhixuanli.github.io/" target="_blank">Zhixuan Li</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="" target="_blank">Weining Ye</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?hl=en&user=6kh0YcEAAAAJ" target="_blank">Juan Terven</a><sup>2</sup>,</span>
              <span class="author-block">
                    <a href="https://www.linkedin.com/in/zachary-bennett-b3199546/" target="_blank">Zachary Bennett</a><sup>2</sup>,</span>
              <br>
              <span class="author-block">
                    <a href="https://www.linkedin.com/in/ying-zheng-1b9070a/" target="_blank">Ying Zheng</a><sup>2</sup>,</span>
              <span class="author-block">
                    <a href="http://www.vie.group/ttj" target="_blank">Tingting Jiang</a><sup>1,*</sup>,</span>
              <span class="author-block">
                    <a href="https://idm.pku.edu.cn/info/1017/1040.htm" target="_blank">Tiejun Huang</a><sup>1,3</sup></span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1 </sup>National Engineering Research Center of Visual Technology, National Key Laboratory for Multimedia Information Processing, School of Computer Science, Peking University, Beijing 100871, China</span>
                    <span class="author-block"><sup>2 </sup>AiFi Inc., California 94010, United States</span>
                    <br>
                    <span class="author-block"><sup>3 </sup>Beijing Academy of Artifcial Intelligence, Beijing 100084, China</span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Corresponding author.</small></span>
                    <br>
                    <br>
                    <span class="author-block"><strong>International Conference on Computer Vision (ICCV), 2023</strong></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                      <span class="link-block">
                        <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Li_MUVA_A_New_Large-Scale_Benchmark_for_Multi-View_Amodal_Instance_Segmentation_ICCV_2023_paper.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

<!--                    &lt;!&ndash; Supplementary PDF link &ndash;&gt;-->
                    <span class="link-block">
                      <a href="https://openaccess.thecvf.com/content/ICCV2023/supplemental/Li_MUVA_A_New_ICCV_2023_supplemental.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://drive.google.com/drive/folders/1T5PNhoWlXBFDwGteVi3x357adM1t2mlo?usp=sharing" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Dataset (Google Drive)</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
<!--                <span class="link-block">-->
<!--                  <a href="https://arxiv.org" target="_blank"-->
<!--                  class="external-link button is-normal is-rounded is-dark">-->
<!--                  <span class="icon">-->
<!--                    <i class="ai ai-arxiv"></i>-->
<!--                  </span>-->
<!--                  <span>arXiv</span>-->
<!--                </a>-->
<!--                </span>-->

            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<!--<section class="hero teaser">-->
<!--  <div class="container is-max-desktop">-->
<!--    <div class="hero-body">-->
<!--      <video poster="" id="tree" autoplay controls muted loop height="100%">-->
<!--        &lt;!&ndash; Your video here &ndash;&gt;-->
<!--        <source src="static/videos/banner_video.mp4"-->
<!--        type="video/mp4">-->
<!--      </video>-->
<!--      <h2 class="subtitle has-text-centered">-->
<!--        Aliquam vitae elit ullamcorper tellus egestas pellentesque. Ut lacus tellus, maximus vel lectus at, placerat pretium mi. Maecenas dignissim tincidunt vestibulum. Sed consequat hendrerit nisl ut maximus. -->
<!--      </h2>-->
<!--    </div>-->
<!--  </div>-->
<!--</section>-->
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-small is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
                  Amodal Instance Segmentation (AIS) endeavors to accurately deduce complete object shapes that are partially or fully occluded.
          However, the inherent ill-posed nature of single-view datasets poses challenges in determining occluded shapes.
          A multi-view framework may help alleviate this problem, as humans often adjust their perspective when encountering occluded objects.
          At present, this approach has not yet been explored by existing methods and datasets.
          To bridge this gap, we propose a new task called <b>M</b>ulti-view <b>A</b>modal <b>I</b>nstance <b>S</b>egmentation (MAIS) and introduce the MUVA dataset,
          the first <b>MU</b>lti-<b>V</b>iew <b>A</b>IS dataset that takes the shopping scenario as instantiation.
          MUVA provides comprehensive annotations, including multi-view amodal/visible segmentation masks, 3D models, and depth maps, making it the largest image-level AIS dataset in terms of both the number of images and instances. Additionally, we propose a new method for aggregating representative features across different instances and views, which demonstrates promising results in accurately predicting occluded objects from one viewpoint by leveraging information from other viewpoints. Besides, we also demonstrate that MUVA can benefit the AIS task in real-world scenarios.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<section class="section hero is-small">
<div class="container is-max-desktop">
  <div class="columns is-centered ">
    <div class="column is-full">
      <div class="content">
        <h2 class="title is-3">Targeting at the ill-posed problem in the AIS task</h2>
        <center>
        <img src="static/images/introduction.jpg" class="center-image blend-img-background"/>
        </center>
        <div class="level-set has-text-justified">
          <p>
      Fig.1: Comparison of the impact of ill-posed problems on amodal prediction in single-view and multi-view input settings.
          (a) In single-view input, ambiguity arises due to multiple candidates for the occluded object.
          (b) Multi-view input helps alleviate ambiguity and improves amodal prediction accuracy.
          </p>
        </div>
      </div>
    </div>
  </div>
</div>
</section>

<section class="section hero is-small is-light">
<div class="container is-max-desktop">
<div class="columns is-centered">
  <div class="column is-full">
    <div class="content">
      <h2 class="title is-3">Dataset Generation Pipeline</h2>
      <center>
      <img src="static/images/dataset_generation_pipeline.jpg"  class="center-image blend-img-background"/>
      </center>
      <div class="level-set has-text-justified">
        <p>
        Fig. 2: The pipeline of dataset generation.
            (a) For each object, 2D images are captured from up, down, left, right, front, and back, respectively. Then 3D artists use the collected images to reconstruct the 3D models.
            (b) For each scene, 3D models are randomly selected and placed with different amounts and orientations. (c) For each scene, six views are used to capture the data, including the RGB images and various annotations.
        </p>
      </div>
    </div>
  </div>
</div>
</div>
</section>


<section class="section hero is-small">
<div class="container is-max-desktop">
<div class="columns is-centered">
  <div class="column is-full">
    <div class="content">
      <h2 class="title is-3">Datasets Comparison</h2>
      <center>
      <img src="static/images/datasets_comparison.jpg" class="center-image blend-img-background"/>
      </center>
      <div class="level-set has-text-justified">
        <p>
        Tab 1: Comparison with existing amodal instance segmentation datasets. # means the number of this item.
            Bold numbers denote the largest one in each column among image-level datasets.
        </p>
      </div>
    </div>
  </div>
</div>
</div>
</section>



<section class="section hero is-small">
<div class="container is-max-desktop">
<div class="columns is-centered">
  <div class="column is-full">
    <div class="content">
      <h2 class="title is-3">Segmentation Results Comparison</h2>
      <center>
      <img src="static/images/exp_methods_compare.jpg" class="center-image blend-img-background"/>
      </center>
      <div class="level-set has-text-justified">
        <p>
Fig 3: Visualization comparisons between BCNet and ours on MUVA, trained with one viewpoint (1V) and six viewpoints (6V).
    For masks, different colors denote different instances, and the same instance in different angles has the same color. Red circles indicate
    regions should be focused. Zoom in for a better view. In the first and third columns, even if a bottle is severely occluded, it can be predicted
    accurately by our method. Moreover, our method trained with 6 views performs better than training with a single view.
        </p>
      </div>
    </div>
  </div>
</div>
</div>
</section>


<!--BibTex citation -->
  <section class="section hero is-small is-light" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
        @inproceedings{li2023muva,
            author={Li, Zhixuan and Ye, Weining and Terven, Juan and Bennett, Zachary and Zheng, Ying and Jiang, Tingting and Huang, Tiejun},
            title={{MUVA}: A New Large-Scale Benchmark for Multi-view Amodal Instance Segmentation in the Shopping Scenario},
            booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
            pages={23504--23513},
            year={2023}
        }
      </code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
