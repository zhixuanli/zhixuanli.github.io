<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>OAFormer</title>
<!--  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">-->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">OAFormer: Learning Occlusion Distinguishable Feature for Amodal Instance Segmentation</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://zhixuanli.github.io/" target="_blank">Zhixuan Li</a><sup>1, 2</sup>,</span>
              <span class="author-block">
                <a href="https://openreview.net/profile?id=~Hyunse_Yoon1" target="_blank">Ruohua Shi</a><sup>2</sup>,</span>
              <span class="author-block">
                    <a href="https://scholar.google.co.kr/citations?user=W1Qo1OUAAAAJ&hl" target="_blank">Tiejun Huang</a><sup>2</sup>,</span>
              <span class="author-block">
                    <a href="https://scholar.google.com.tw/citations?user=D_S41X4AAAAJ" target="_blank">Tingting Jiang</a><sup>1, 2*</sup></span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1</sup>Advanced Institute of Information Technology, Peking University, Hangzhou, China,</span><br>
                    <span class="author-block"><sup>2</sup>National Engineering Research Center of Visual Technology, School of Computer Science, Peking University, Beijing, China</span><br>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Corresponding author.</small></span>
                    <br>
                    <span class="author-block"><strong>ICASSP 2023</strong></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://ieeexplore.ieee.org/abstract/document/10096534" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                            <!-- ArXiv abstract Link -->
<!--                      <span class="link-block">-->
<!--                        <a href="https://arxiv.org/abs/2503.10225" target="_blank"-->
<!--                        class="external-link button is-normal is-rounded is-dark">-->
<!--                        <span class="icon">-->
<!--                          <i class="ai ai-arxiv"></i>-->
<!--                        </span>-->
<!--                        <span>arXiv</span>-->
<!--                      </a>-->
<!--                    </span>-->


            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <center>
      <img src="static/images/motivation_image.png" class="center-image blend-img-background" width="600" height="300"/>
      <h2 class="subtitle has-text-justified">
        Typical cases of the occlusion confusing problem. (a) Occlude bottle is regarded as unoccluded and resulting in wrong prediction. (b) Unoccluded chocolate is regarded as occluded and predicted wrongly.
      </h2>
      </center>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-small is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
          The Amodal Instance Segmentation (AIS) task aims to infer the complete mask of occluded instance. Under many circumstances, existing methods treat occluded objects as unoccluded ones, and vice versa, leading to inaccurate predictions. This is because existing AIS methods do not explicitly utilize the occlusion rates of each object as supervision. However, occlusion information is critical for the methods to recognize whether the target objects are occluded. Hence we believe it is vital for the method to be distinguishable about the degree of occlusion for each instance. In this paper, a simple yet effective Occlusion-aware transformer-based model, OAFormer, is proposed for accurate amodal instance segmentation. The goal of OAFormer is to learn the occlusion discriminative features. Novel components are proposed to enable OAFormer to be occlusion distinguishable. We conduct extensive experiments on two challenging AIS datasets to evaluate the effectiveness of our method. OAFormer outperforms state-of-the-art methods by large margins.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->




<section class="section hero is-small">
<div class="container is-max-desktop">
<div class="columns is-centered">
  <div class="column is-full">
    <div class="content">
      <h2 class="title is-3">The Proposed OAFormer Approach</h2>
      <center>
      <img src="static/images/overall_architecture.png" class="center-image blend-img-background"/>
      </center>
      <div class="level-set has-text-justified">
        <p>
        Overview of the proposed OAFormer. OAFormer takes an image as the input. After extracting the features by the Encoder and the Cascaded Global Decoder, the Occlusion Distinguish Module predicts the occlusion rates of each target objects and embeds occlusion information into the attention masks. Finally, the Amodal Decoder takes the occlusion-aware attention masks and queries as input, and outputs the predicted amodal masks.
        </p>
      </div>
    </div>
  </div>
</div>
</div>
</section>


<section class="section hero is-small is-light">
<div class="container is-max-desktop">
<div class="columns is-centered">
  <div class="column is-full">
    <div class="content">
      <h2 class="title is-3">Experimental Results</h2>
      <center>
      <img src="static/images/exp_res.png" class="center-image blend-img-background"/>
      </center>
      <div class="level-set has-text-justified">
        <p>
        Comparison with state-of-the-art methods on the D2SA and COCOA-cls datasets. For supervision, “bbox” means amodal bounding box, “mask” means amodal masks, and “cls” means class labels. For each metric, the bold performance is the best, and the second-best is underlined.
        </p>
      </div>
    </div>
  </div>
</div>
</div>
</section>

<!--BibTex citation -->
  <section class="section hero is-small" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
      @inproceedings{li2023oaformer,
          title={{OAFormer}: Learning Occlusion Distinguishable Feature for Amodal Instance Segmentation},
          author={Li, Zhixuan and Shi, Ruohua and Huang, Tiejun and Jiang, Tingting},
          booktitle={International Conference on Acoustics, Speech, and Signal Processing},
          pages={1--5},
          year={2023}
      }
      </code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
