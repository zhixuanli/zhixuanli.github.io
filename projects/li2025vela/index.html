<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>VELA</title>
<!--  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">-->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Single Point, Full Mask: Velocity-Guided Level Set Evolution for End-to-End Amodal Segmentation</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://zhixuanli.github.io/" target="_blank">Zhixuan Li</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?hl=en&user=iDyKEuwAAAAJ&view_op=list_works" target="_blank">Yujia Liu</a><sup>2,3,4</sup>,</span>
              <span class="author-block">
                    <a href="https://scholar.google.com/citations?hl=en&user=33MsN_4AAAAJ&view_op=list_works" target="_blank">Chen Hui</a><sup>5</sup>,</span>
              <span class="author-block">
                    <a href="https://scholar.google.com/citations?user=D_S41X4AAAAJ&hl=en" target="_blank">Weisi Lin</a><sup>1*</sup></span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1</sup>Nanyang Technological University, Singapore,</span><br>
                    <span class="author-block"><sup>2</sup>School of Computer Science, Peking University, Beijing, China</span><br>
                    <span class="author-block"><sup>3</sup>National Key Laboratory for Multimedia Information Processing, Peking University, Beijing, China</span><br>
                    <span class="author-block"><sup>4</sup>National Engineering Research Center of Visual Technology, Peking University, Beijing, China</span><br>
                    <span class="author-block"><sup>5</sup>Nanjing University of Information Science and Technology, China</span><br>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Corresponding author.</small></span>
                    <br>
                    <span class="author-block"><strong>Preprint</strong></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/2508.01661" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                            <!-- ArXiv abstract Link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/abs/2508.01661" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="ai ai-arxiv"></i>
                        </span>
                        <span>arXiv</span>
                      </a>
                    </span>

<!--                    &lt;!&ndash; Supplementary PDF link &ndash;&gt;-->
<!--                    <span class="link-block">-->
<!--                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"-->
<!--                      class="external-link button is-normal is-rounded is-dark">-->
<!--                      <span class="icon">-->
<!--                        <i class="fas fa-file-pdf"></i>-->
<!--                      </span>-->
<!--                      <span>Supplementary</span>-->
<!--                    </a>-->
<!--                  </span>-->

                  <!-- Github link -->
<!--                  <span class="link-block">-->
<!--                    <a href="" target="_blank"-->
<!--                    class="external-link button is-normal is-rounded is-dark">-->
<!--                    <span class="icon">-->
<!--                      <i class="fab fa-github"></i>-->
<!--                    </span>-->
<!--                    <span>Code (Will be released)</span>-->
<!--                  </a>-->
<!--                </span>-->

<!--                <span class="link-block">-->
<!--                  <a href="" target="_blank"-->
<!--                  class="external-link button is-normal is-rounded is-dark">-->
<!--                  <span class="icon">-->
<!--                    <i class="fab fa-github"></i>-->
<!--                  </span>-->
<!--                  <span>Dataset (Will be released)</span>-->
<!--                </a>-->
<!--              </span>-->


            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>



<!-- Paper abstract -->
<section class="section hero is-small is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Amodal segmentation aims to recover complete object shapes, including occluded regions with no visual appearance, whereas conventional segmentation focuses solely on visible areas. Existing methods typically rely on strong prompts, such as visible masks or bounding boxes, which are costly or impractical to obtain in real-world settings. While recent approaches such as the Segment Anything Model (SAM) support point-based prompts for guidance, they often perform direct mask regression without explicitly modeling shape evolution, limiting generalization in complex occlusion scenarios. Moreover, most existing methods suffer from a black-box nature, lacking geometric interpretability and offering limited insight into how occluded shapes are inferred. To deal with these limitations, we propose VELA, an end-to-end VElocity-driven Level-set Amodal segmentation method that performs explicit contour evolution from point-based prompts. VELA first constructs an initial level set function from image features and the point input, which then progressively evolves into the final amodal mask under the guidance of a shape-specific motion field predicted by a fully differentiable network. This network learns to generate evolution dynamics at each step, enabling geometrically grounded and topologically flexible contour modeling. Extensive experiments on COCOA-cls, D2SA, and KINS benchmarks demonstrate that VELA outperforms existing strongly prompted methods while requiring only a single-point prompt, validating the effectiveness of interpretable geometric modeling under weak guidance.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->




<section class="section hero is-small">
<div class="container is-max-desktop">
<div class="columns is-centered">
  <div class="column is-full">
    <div class="content">
      <center>
      <h2 class="title is-3">Motivation</h2>
      <img src="static/images/motivation.jpg"  class="center-image blend-img-background"/>
      </center>
      <div class="level-set has-text-justified">
        <p>
           Comparison of amodal segmentation paradigms. (a) Conventional methods rely on strong prompts like bounding boxes or visible masks. (b) Recent approaches support point prompts but directly regress masks without explicit shape reasoning, resulting in limited robustness under complex occlusions. (c) VELA introduces explicit shape evolution via Level Set, effectively exploiting a single-point prompt for interpretable amodal segmentation.
        </p>
      </div>
    </div>
  </div>
</div>
</div>
</section>


<section class="section hero is-small is-light">
<div class="container is-max-desktop">
<div class="columns is-centered">
  <div class="column is-full">
    <div class="content">
      <center>
      <h2 class="title is-3">The Proposed VELA Method</h2>
      <img src="static/images/network_architecture.jpg"  class="center-image blend-img-background"/>
      </center>
      <div class="level-set has-text-justified">
        <p>
           Overall architecture of the proposed VELA. (a) Level Set Function (LSF) Initialization: Given an input image $I$, the Vision Backbone extracts global image features, while the Prompt Encoder processes the point-level prompt $P$ to generate point embeddings $e_P$ that encode coarse localization of the target object. The Initial LSF Decoder then predicts the initial level set function $\phi_0$ using both the image features and point embeddings. (b) Velocity-Guided Shape Evolution: Starting from $\phi_0$, the shape evolves over $T$ steps. At each step $i$, the current level set function $\phi_i$ is fed into the Shape-Specific Velocity Generator to dynamically produce a velocity field $V^n_i$, which guides the contour’s deformation towards the final shape. The updated $\phi_{i+1}$ is then regularized and used for the next step. After $T$ steps, the final level set function $\phi_T$ is thresholded to obtain the predicted amodal mask $\hat{M}_a$.
        </p>
      </div>
    </div>
  </div>
</div>
</div>
</section>


<section class="section hero is-small">
<div class="container is-max-desktop">
<div class="columns is-centered">
  <div class="column is-full">
    <div class="content">
      <center>
      <h2 class="title is-3">Qualitative Results</h2>
      <img src="static/images/qualitative_results.jpg" class="center-image blend-img-background"/>
      </center>
      <div class="level-set has-text-justified">
        <p>
           Visualization results of VELA. The green dot denotes the input point prompt. Best viewed in color.
        </p>
      </div>
    </div>
  </div>
</div>
</div>
</section>

<!--BibTex citation -->
  <section class="section hero is-small is-light" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
          @inproceedings{li2025vela,
                title={Single Point, Full Mask: Velocity-Guided Level Set Evolution for End-to-End Amodal Segmentation},
                author={Li, Zhixuan and Liu, Yujia and Hui, Chen and Lin, Weisi},
                booktitle={arXiv:2508.01661},
                year={2025}
          }
      </code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
